# -*- coding: utf-8 -*-
"""app.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1gpUR14dlUpGHOSbkwnXTivLj5NH_cy-t
"""

import pandas as pd
import nltk
import re
import ast
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from nltk.tokenize import word_tokenize

# ==============================================================================
# Step 1: Download NLTK Data Packages
# ==============================================================================

print("✅ Importing libraries and downloading NLTK data...")
nltk.download('stopwords', quiet=True)
nltk.download('punkt', quiet=True)
nltk.download('wordnet', quiet=True)
nltk.download('punkt_tab')
print("✅ Downloads complete.")


# ==============================================================================
# Step 2: Load Data from File Path
# ==============================================================================

file_path = '/content/all_job_post.csv' # <-- PASTE YOUR PATH HERE

try:
    df = pd.read_csv(file_path)
    print(f"✅ Successfully loaded '{file_path}'.")
except FileNotFoundError:
    print(f"❌ ERROR: File not found at '{file_path}'.")
    print("Please make sure you have uploaded the file and the path is correct.")

    exit()


# ==============================================================================
# Step 3: Define and Apply Preprocessing Functions
# ==============================================================================
print("\n⏳ Starting data preprocessing...")

# Initialize the lemmatizer and English stopwords set
lemmatizer = WordNetLemmatizer()
stop_words = set(stopwords.words('english'))

def preprocess_text(text):
    """Cleans and tokenizes the job description text."""
    if not isinstance(text, str):
        return []
    text = text.lower()
    text = re.sub(r'\s+', ' ', text)  # Collapse whitespace
    text = re.sub(r'[^\w\s]', '', text)  # Remove punctuation
    tokens = word_tokenize(text)
    processed_tokens = [lemmatizer.lemmatize(word) for word in tokens if word not in stop_words]
    return processed_tokens

def process_skill_set(skill_string):
    """Parses and cleans the string representation of a skill list."""
    try:
        skills = ast.literal_eval(skill_string)
        return [skill.lower().strip() for skill in skills]
    except (ValueError, SyntaxError, TypeError):
        return []

# Drop rows where essential columns are empty to prevent errors
df.dropna(subset=['job_description', 'job_skill_set'], inplace=True)

# Apply the preprocessing functions to the relevant columns
df['processed_description'] = df['job_description'].apply(preprocess_text)
df['processed_job_skill_set'] = df['job_skill_set'].apply(process_skill_set)

print("✅ Preprocessing complete.")


# ==============================================================================
# Step 4: Final Inspection and Saving
# ==============================================================================
# Create a final, clean DataFrame with the columns we need
final_df = df[[
    'job_title',
    'processed_description',
    'processed_job_skill_set'
]].copy()

print("\n--- Processed Data Preview (First 5 Rows) ---")
print(final_df.head().to_string())

# Save the processed data to a new CSV file for the next phase
output_filename = 'processed_jobs.csv'
final_df.to_csv(output_filename, index=False)

print(f"\n\n✅ All steps are complete! The processed data has been saved as '{output_filename}'.")
print("You can find this file in the Colab file explorer, ready for the next step.")

import pandas as pd
import ast
from collections import Counter
import re

# ==============================================================================
# Step 1: Load the Dataset
# ==============================================================================
file_path = '/content/all_job_post.csv'

try:
    df = pd.read_csv(file_path)
    print(f"✅ Successfully loaded '{file_path}'.")
except FileNotFoundError:
    print(f"❌ ERROR: File not found at '{file_path}'.")
    exit()

# Drop rows where the skill set is missing
df.dropna(subset=['job_skill_set'], inplace=True)


# ==============================================================================
# Step 2: Extract and Count All Ground Truth Skills
# ==============================================================================
print("\n⏳ Analyzing all skills from the dataset...")

all_skills_list = []

# Iterate over the 'job_skill_set' column
for index, row in df.iterrows():
    skill_string = row['job_skill_set']
    try:
        # Safely parse the string representation of a list
        skills = ast.literal_eval(skill_string)
        # Clean and add skills to our master list
        cleaned_skills = [skill.lower().strip() for skill in skills]
        all_skills_list.extend(cleaned_skills)
    except (ValueError, SyntaxError):
        # Skip rows where the format is incorrect
        continue

# Count the frequency of each skill
skill_counts = Counter(all_skills_list)
total_unique_skills = len(skill_counts)

print(f"✅ Analysis Complete. Found {total_unique_skills} unique skills.")


# ==============================================================================
# Step 3: Display the Most Common Skills
# ==============================================================================
# This list is the foundation for our new, powerful SKILL_DB
print("\n--- Top 150 Most Common Skills in the Dataset ---")
for skill, count in skill_counts.most_common(150):
    print(f"{skill:<40} | Count: {count}")


# ==============================================================================
# Step 4: Create the New Data-Driven SKILL_DB
# ==============================================================================
# Get the top 200 skills to ensure good coverage
top_skills_set = {skill for skill, count in skill_counts.most_common(200)}

# Define categories for classification
hard_skill_keywords = [
    'sql', 'python', 'java', 'c++', 'javascript', 'r', 'c#', 'aws', 'azure', 'gcp', 'cloud',
    'docker', 'kubernetes', 'git', 'api', 'data', 'analysis', 'analytics', 'management',
    'excel', 'power bi', 'tableau', 'saas', 'crm', 'erp', 'html', 'css', 'react', 'angular',
    'node.js', 'project management', 'agile', 'scrum', 'devops', 'testing', 'automation',
    'linux', 'windows', 'security', 'network', 'database', 'mongodb', 'mysql', 'postgresql',
    'machine learning', 'ai', 'tensorflow', 'pytorch', 'scikit-learn', 'pandas', 'numpy',
    'marketing', 'seo', 'sem', 'content', 'advertising', 'salesforce', 'sap', 'oracle'
]

def categorize_skill(skill):
    for keyword in hard_skill_keywords:
        if re.search(r'\b' + re.escape(keyword) + r'\b', skill):
            return 'Hard Skills'
    return 'Soft Skills'

DATA_DRIVEN_SKILL_DB = {
    'Hard Skills': sorted([skill for skill in top_skills_set if categorize_skill(skill) == 'Hard Skills']),
    'Soft Skills': sorted([skill for skill in top_skills_set if categorize_skill(skill) == 'Soft Skills'])
}

print("\n\n--- Generated Data-Driven SKILL_DB ---")
print("\n# --- Hard Skills ---")
print(f"'{', '.join(DATA_DRIVEN_SKILL_DB['Hard Skills'])}'")
print("\n# --- Soft Skills ---")
print(f"'{', '.join(DATA_DRIVEN_SKILL_DB['Soft Skills'])}'")

import pandas as pd
import ast
import re

# ==============================================================================
# Step 1: Load the Processed Data from a File Path
# ==============================================================================
file_path = '/content/processed_jobs.csv'

try:
    df = pd.read_csv(file_path)
    print(f"✅ Successfully loaded '{file_path}'.")
except FileNotFoundError:
    print(f"❌ ERROR: File not found at '{file_path}'.")
    print("Please make sure the file is uploaded and the path is correct.")
    # Stop execution if the file isn't found
    exit()


# The CSV columns that are lists get read as strings. We need to convert them back.
df['processed_description'] = df['processed_description'].apply(ast.literal_eval)
df['processed_job_skill_set'] = df['processed_job_skill_set'].apply(ast.literal_eval)

print("✅ Data loaded and columns converted successfully.")


# ==============================================================================
# Step 2: Curate the Skill Dictionary 📚
# ==============================================================================
# The new DATA_DRIVEN_SKILL_DB based on our dataset analysis
DATA_DRIVEN_SKILL_DB = {
    'Hard Skills': [
        'account management', 'budget management', 'cash flow management', 'change management',
        'client relationship management', 'compensation management', 'crm', 'crm software',
        'crm systems', 'customer relationship management', 'data analysis', 'data analytics',
        'data entry', 'data management', 'erp systems', 'excel', 'financial analysis',
        'financial management', 'hr management', 'human resources management', 'inventory management',
        'it management', 'market analysis', 'marketing', 'microsoft excel', 'ms excel',
        'network security', 'payroll management', 'performance management', 'pipeline management',
        'power bi', 'project management', 'relationship management', 'risk management', 'saas',
        'sales management', 'salesforce', 'sap', 'sql', 'stakeholder management', 'talent management',
        'team management', 'territory management', 'time management', 'vendor management'
    ],
    'Soft Skills': [
        'accountability', 'accounting', 'accounting principles', 'accounts payable', 'active directory',
        'adaptability', 'analytical skills', 'analytical thinking', 'attention to detail', 'auditing',
        'b2b sales', 'background checks', 'benefits administration', 'budgeting', 'business acumen',
        'business development', 'cash handling', 'coaching', 'cold calling', 'collaboration',
        'communication', 'communication skills', 'compliance', 'computer skills', 'confidentiality',
        'conflict resolution', 'consultative selling', 'continuous improvement', 'continuous learning',
        'contract negotiation', 'cpa', 'creativity', 'critical thinking', 'cultural awareness',
        'curiosity', 'customer engagement', 'customer focus', 'customer service',
        'customer service orientation', 'cybersecurity', 'decision making', 'decision-making',
        'detail-oriented', 'discretion', 'documentation', 'empathy', 'employee engagement',
        'employee onboarding', 'employee relations', 'employment law', 'employment laws', 'engagement',
        'enthusiasm', 'entrepreneurial mindset', 'finance', 'financial forecasting', 'financial modeling',
        'financial planning', 'financial reporting', 'flexibility', 'forecasting', 'gaap',
        'goal-oriented', 'hris', 'hris systems', 'human resources', 'independence',
        'independent work', 'influence', 'influencing', 'influencing skills', 'information technology',
        'initiative', 'innovation', 'integrity', 'internal controls', 'interpersonal skills', 'jira',
        'judgment', 'lead generation', 'leadership', 'listening skills', 'market research', 'mentoring',
        'mentorship', 'merchandising', 'microsoft office', 'microsoft office suite',
        'microsoft powerpoint', 'microsoft word', 'motivation', 'ms office', 'ms office suite',
        'multi-tasking', 'multitasking', 'negotiation', 'networking', 'office 365', 'onboarding',
        'organization', 'organizational design', 'organizational development', 'organizational skills',
        'payroll administration', 'payroll processing', 'persuasion', 'pmp certification',
        'policy development', 'positive attitude', 'powerpoint', 'presentation skills', 'prioritization',
        'proactivity', 'problem solving', 'problem-solving', 'process improvement', 'product knowledge',
        'professionalism', 'prospecting', 'recruiting', 'recruitment', 'regulatory compliance',
        'relationship building', 'reporting', 'results-oriented', 'risk assessment', 'sales',
        'sales experience', 'sales forecasting', 'sales strategies', 'sales strategy',
        'sales strategy development', 'sales techniques', 'self-motivated', 'self-motivation',
        'self-starter', 'sharepoint', 'stakeholder engagement', 'strategic planning',
        'strategic thinking', 'succession planning', 'talent acquisition', 'team building',
        'team collaboration', 'team leadership', 'teamwork', 'technical support', 'training',
        'training and development', 'troubleshooting', 'valid driver\'s license', 'visual merchandising',
        'work ethic', 'workday', 'workforce planning'
    ]
}

all_skills = set(DATA_DRIVEN_SKILL_DB['Hard Skills'] + DATA_DRIVEN_SKILL_DB['Soft Skills'])

print(f"✅ Data-Driven Skill dictionary created with {len(all_skills)} unique skills.")


# ==============================================================================
# Step 3: Implement the Skill Extraction Logic 🧠
# ==============================================================================
def extract_skills(description_tokens):
    """
    Extracts skills from a list of preprocessed description tokens.
    """
    description_text = ' '.join(description_tokens)
    found_skills = set()

    for skill in all_skills:
        pattern = r"\b" + re.escape(skill) + r"\b"
        if re.search(pattern, description_text, re.IGNORECASE):
            found_skills.add(skill)

    return list(found_skills)

print("\n⏳ Applying skill extraction logic to the dataset...")

# Apply the function to the 'processed_description' column
df['extracted_skills'] = df['processed_description'].apply(extract_skills)

print("✅ Skill extraction complete.")


# ==============================================================================
# Step 4: Review the Results and Save 💾
# ==============================================================================
# Compare the ground truth skills with our extracted skills
review_df = df[[
    'job_title',
    'processed_job_skill_set',
    'extracted_skills'
]].copy()

print("\n--- Extraction Results Preview (First 5 Rows) ---")
print(review_df.head().to_string())


# Save the new DataFrame. This file will be used for the Validation phase.
output_filename = 'jobs_with_extracted_skills.csv'
df.to_csv(output_filename, index=False)

print(f"\n\n✅ All steps are complete! Data saved to '{output_filename}'.")
print("This file is ready for the **Validation and Evaluation** phase.")

import pandas as pd
import numpy as np
import ast
from collections import Counter

# ==============================================================================
# Step 1: Load the Data with Extracted Skills
# ==============================================================================
file_path = '/content/jobs_with_extracted_skills.csv'

try:
    df = pd.read_csv(file_path)
    print(f"✅ Successfully loaded '{file_path}'.")
except FileNotFoundError:
    print(f"❌ ERROR: File not found at '{file_path}'.")
    print("Please ensure the file is uploaded and the path is correct.")
    exit()

# Convert the string representations of lists back into actual lists
for col in ['processed_description', 'processed_job_skill_set', 'extracted_skills']:
    df[col] = df[col].apply(ast.literal_eval)

print("✅ Data loaded and columns converted successfully.")


# ==============================================================================
# Step 2: Calculate Metrics for Each Job Description
# ==============================================================================
# We will calculate True Positives, False Positives, and False Negatives for each row.

results = []
all_false_positives = []
all_false_negatives = []

for index, row in df.iterrows():
    ground_truth = set(row['processed_job_skill_set'])
    extracted = set(row['extracted_skills'])

    # True Positives: Skills in both sets
    tp = len(ground_truth.intersection(extracted))

    # False Positives: Skills extracted but not in ground truth
    fp = len(extracted.difference(ground_truth))

    # False Negatives: Skills in ground truth but not extracted
    fn = len(ground_truth.difference(extracted))

    # Store the actual FP and FN skills for later analysis
    all_false_positives.extend(list(extracted.difference(ground_truth)))
    all_false_negatives.extend(list(ground_truth.difference(extracted)))

    # Calculate metrics for this row
    precision = tp / (tp + fp) if (tp + fp) > 0 else 0
    recall = tp / (tp + fn) if (tp + fn) > 0 else 0
    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0

    results.append({'precision': precision, 'recall': recall, 'f1_score': f1_score})

# Create a DataFrame from the results
results_df = pd.DataFrame(results)
print("\n✅ Metrics calculated for each job description.")


# ==============================================================================
# Step 3: Display Overall Performance Metrics 📊
# ==============================================================================
# Calculate the average scores across all job descriptions

avg_precision = results_df['precision'].mean()
avg_recall = results_df['recall'].mean()
avg_f1_score = results_df['f1_score'].mean()

print("\n--- Overall System Performance ---")
print(f"🎯 Average Precision: {avg_precision:.2%}")
print(f"🔍 Average Recall:    {avg_recall:.2%}")
print(f"⚖️ Average F1-Score:  {avg_f1_score:.2%}")
print("------------------------------------")


# ==============================================================================
# Step 4: Analyze Errors for Refinement 🧠
# ==============================================================================
# This is the most important part for improving your model.
# By finding the most common errors, you know what to fix in your skill dictionary.

fp_counts = Counter(all_false_positives)
fn_counts = Counter(all_false_negatives)

print("\n--- Top 15 Most Common False Positives (Incorrectly Extracted) ---")
print("These skills were extracted but were NOT in the ground truth.")
print("Consider if these are truly errors or if your dictionary is too broad.\n")
for skill, count in fp_counts.most_common(15):
    print(f"{skill:<20} | Count: {count}")

print("\n--- Top 15 Most Common False Negatives (Missed Skills) ---")
print("These skills were in the ground truth but were NOT extracted.")
print("💡 ACTION: Add these skills (or their variations) to your SKILL_DB to improve recall.\n")
for skill, count in fn_counts.most_common(25):
    print(f"{skill:<20} | Count: {count}")
